# -*- coding: utf-8 -*-
"""3b-weather_location_grid_creation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EUBcKqA4nywMagGbdbapXsnEyy252tq6

## Creating weather location grid table
Inputs:
- {province}_location_grid_wooded.csv.zip
- {province}_location_grid_usefulness.csv.zip
- {province_lowercase}_grid_system_with_station.csv.zip
- current_station_inventory.csv.zip

1. Import existing grid files.
2. Eliminate non-useful grids.
3. Create new dataframe by iterating through grids and weather for that specific grid.

## Setup
"""

province = 'BC'

root_path = '/content/gdrive/My Drive/Capstone Public Folder/Data/'

narrow_grids = False

!pip install aiohttp

# Imports
import pandas as pd
import numpy as np
import io
from google.colab import files, drive
import zipfile
import shutil
import time
import requests
from io import StringIO

import asyncio
import aiohttp

# Mount Google Drive
drive.mount('/content/gdrive')

"""## Relevant Files"""

if narrow_grids:
  required_files = [
    f'{province} Location/Ivan_NarrowedData/{province}_location_grid_wooded.csv.zip',
    f'{province} Location/Shima_NarrowedData/{province}_location_grid_usefulness.csv.zip'
  ]

  for local_path in required_files:
    file_path = root_path + local_path
    with zipfile.ZipFile(file_path,"r") as zip_ref:
      zip_ref.extractall('.')

# Import all files into pandas dataframes
if narrow_grids:
  location_grid_wooded = pd.read_csv(f'{province}_location_grid_wooded.csv')
  location_grid_boreal = pd.read_csv(f'{province}_location_grid_usefulness.csv')
location_grid_stations = pd.read_csv(root_path + f'{province} Location/{province}_grid_system_with_station.csv.gz')
weather_stations = pd.read_csv(root_path + 'Weather - Stations/current_station_inventory.csv.gz')

"""## Daily Weather Call"""

start_year = 2008
end_year = 2018
station_list = location_grid_stations['STATION_ID'].unique()
dfs = []

async def get_daily_weather(session, stationID, year):
  url = "http://climate.weather.gc.ca/climate_data/bulk_data_e.html?format=csv&" +\
            f"stationID={stationID}&Year={year}&Month=1&Day=14&timeframe=2&submit=Download+Data"
  async with session.get(url) as response:
    byte_code = await response.content.read()
    string_data = str(byte_code, 'utf-8')
    data = StringIO(string_data)
    new_df = pd.read_csv(data)
    new_df['STATION_ID'] = stationID
    dfs.append(new_df)

async def caller():
  async with aiohttp.ClientSession() as session:
    tasks = []
    index = 0
    for stationID in station_list:
      for year in range(start_year, end_year+1):
        tasks.append(get_daily_weather(session, stationID, year))
      
    await asyncio.gather(*tasks, return_exceptions=True)
    return pd.concat(dfs, ignore_index=True)

start = time.time( )
loop = asyncio.get_event_loop()
df = loop.run_until_complete(caller())
end = time.time()
print(end - start)

# Narrows weather data by months
filter_months = range(1,13) #[7,8,9]  # no more filter used
if False:
  if len(filter_months) > 0:
      df = df.loc[df['Month'].isin(filter_months)]

df['DATETIME']= pd.to_datetime(df['Date/Time']).dt.normalize()

daily_weather_station_data = df
daily_weather_station_data

"""## Hourly Weather Calls"""

hourly_dfs = []

async def get_hourly_weather(session, stationID, year, month):
  url = "http://climate.weather.gc.ca/climate_data/bulk_data_e.html?format=csv&" +\
            f"stationID={stationID}&Year={year}&Month={month}&Day=14&timeframe=1&submit=Download+Data"
  async with session.get(url) as response:
    byte_code = await response.content.read()
    string_data = str(byte_code, 'utf-8')
    data = StringIO(string_data)
    new_df = pd.read_csv(data)
    new_df['STATION_ID'] = stationID
    hourly_dfs.append(new_df)

async def caller():
  async with aiohttp.ClientSession() as session:
    tasks = []
    index = 0
    for stationID in station_list:
      for year in range(start_year, end_year+1):
        for month in filter_months:
          tasks.append(get_hourly_weather(session, stationID, year, month))
      
    await asyncio.gather(*tasks, return_exceptions=True)
    return pd.concat(hourly_dfs, ignore_index=True)

start = time.time( )
loop = asyncio.get_event_loop()
hourly_weather_data = loop.run_until_complete(caller())
end = time.time()
print(end - start)

# Are stations missing from hourly weather?
np.setdiff1d(station_list, hourly_weather_data['STATION_ID'].unique())

# Only keep hot times (12:00 - 4:00 pm)
filter_times = ['12:00', '13:00', '14:00', '15:00', '16:00']
if len(filter_times) > 0:
  hourly_weather_data = hourly_weather_data.loc[hourly_weather_data['Time'].isin(filter_times)]

hourly_weather_slimmed = hourly_weather_data
hourly_weather_slimmed['DATETIME']= pd.to_datetime(hourly_weather_data['Date/Time']).dt.normalize()#pd.to_datetime(hourly_weather_data['Date/Time']).astype('datetime64[ns]') 
hourly_weather_slimmed = hourly_weather_slimmed.rename(columns={'Temp (°C)': 'TEMP_12_4', 'Dew Point Temp (°C)':'DEW_TEMP_12_4', 'Rel Hum (%)': 'REL_HUM_12_4'})
hourly_weather_slimmed = hourly_weather_slimmed[['STATION_ID', 'DATETIME', 'TEMP_12_4', 'DEW_TEMP_12_4', 'REL_HUM_12_4']]
hourly_weather_slimmed = hourly_weather_slimmed.groupby(['STATION_ID','DATETIME']).mean()
hourly_weather_slimmed = hourly_weather_slimmed.reset_index()

hourly_weather_slimmed

null_columns=hourly_weather_slimmed.columns[hourly_weather_slimmed.isnull().any()]
hourly_weather_slimmed[null_columns].isnull().sum()

# Add back up here
hourly_weather_slimmed_with_backup = hourly_weather_slimmed.copy()
missing_stations = np.setdiff1d(location_grid_stations['STATION_ID'].unique(), hourly_weather_slimmed['STATION_ID'].unique())
backup_present = []
backup_missing = []

# Check which stations have backup hourly weather data
for stationID in missing_stations:
  backup_stationID = location_grid_stations[location_grid_stations['STATION_ID'] == stationID]['BACKUP_STATION_ID'].iloc[0]
  if backup_stationID not in missing_stations:
    backup_present.append(stationID)
  else:
    backup_missing.append(stationID)

print(f'Backup present for: {backup_present}')
print(f'Backup missing for: {backup_missing}')

# If backup is present, add it to hourly weather data
for stationID in backup_present:
  backup_stationID = location_grid_stations[location_grid_stations['STATION_ID'] == stationID]['BACKUP_STATION_ID'].iloc[0]
  hourly_weather_for_backup_station = hourly_weather_slimmed[hourly_weather_slimmed['STATION_ID'] == backup_stationID].copy()
  hourly_weather_for_backup_station['STATION_ID'] = stationID
  hourly_weather_slimmed_with_backup = pd.concat([hourly_weather_slimmed_with_backup, hourly_weather_for_backup_station])

"""## Merging Daily and Hourly"""

merged_weather = pd.merge(left=daily_weather_station_data, right=hourly_weather_slimmed_with_backup, on=['STATION_ID','DATETIME'])
merged_weather

weather_stations['STATION_ID'] = weather_stations['Station ID']

# Get daily weather ready for merging
daily_weather_with_station = pd.merge(weather_stations, merged_weather, on=['STATION_ID'])
daily_weather_with_station

# Problem weather stations
unmatched = daily_weather_with_station.loc[daily_weather_with_station['STATION_ID'] != daily_weather_with_station['Station ID']]
unmatched[['Station Name', 'STATION_ID', 'Station ID']].drop_duplicates(keep='last')

"""## Filling in missing values"""

daily_weather_stations_missing = np.setdiff1d(station_list, merged_weather['STATION_ID'])
daily_weather_stations_missing

# Find weather stations that have empty values
all_condition = True
or_condition = False
for col in [
 'Max Temp (°C)',
 'Min Temp (°C)',
 'Mean Temp (°C)',
 'Heat Deg Days (°C)',
 'Cool Deg Days (°C)',
 'Total Rain (mm)',
 'Total Snow (cm)',
 'Total Precip (mm)',
 'Snow on Grnd (cm)',
 'Dir of Max Gust (10s deg)',
 'Spd of Max Gust (km/h)',
 'TEMP_12_4',
 'DEW_TEMP_12_4',
 'REL_HUM_12_4']:
  is_val_null = daily_weather_with_station[col].isnull()
  all_condition = all_condition & is_val_null
  or_condition = or_condition | is_val_null

weather_stations_missing_some_data = daily_weather_with_station[or_condition]['STATION_ID'].value_counts().reset_index().rename({'index': 'STATION_ID', 'STATION_ID': 'AMOUNT_SOME_MISSING'}, axis=1)
weather_stations_missing_all_data = daily_weather_with_station[all_condition]['STATION_ID'].value_counts().reset_index().rename({'index': 'STATION_ID', 'STATION_ID': 'AMOUNT_ALL_MISSING'}, axis=1)
print(f'Total weather stations: {len(daily_weather_with_station["STATION_ID"].unique())}')

weather_stations_missing_all_data

weather_stations_missing_some_data

# Conclusion - all weather stations are missing some data
# However for weather stations missing over days of ALL data, we'll use backup weather
threshold = 45*(end_year - start_year)
bad_stations = weather_stations_missing_all_data.loc[weather_stations_missing_all_data['AMOUNT_ALL_MISSING'] > threshold]['STATION_ID']

def is_backup_needed(row):
  if row['STATION_ID'] in bad_stations.unique() and row['BACKUP_STATION_ID'] not in bad_stations.unique():
    row['STATION_ID'] = row['BACKUP_STATION_ID']
  return row

location_grid_stations_using_backup = location_grid_stations.apply(lambda row: is_backup_needed(row), axis=1)
bad_stations

for bad_station in bad_stations:
  if(len(location_grid_stations_using_backup[location_grid_stations_using_backup['STATION_ID'] == bad_station]) > 0):
    print(f'Bad_station: {bad_station} dooes not have replacement.')

weather_location_grid = pd.merge(location_grid_stations_using_backup , daily_weather_with_station, on='STATION_ID')

# Clean up table
weather_location_grid.rename(columns={'KEY': 'LOCATION_KEY'}, inplace=True)
weather_location_grid['LOCATION_DATE_KEY'] = weather_location_grid['LOCATION_KEY'] + '|' + weather_location_grid['Date/Time']
weather_location_grid.head()

weather_location_grid.describe()

def createLocationYearKey(locationKey, year):
    return locationKey + "|" + str(year)

weather_location_grid['LOCATION_YEAR_KEY'] = np.vectorize(createLocationYearKey)(weather_location_grid['LOCATION_KEY'], weather_location_grid['Year'])
weather_location_grid

len(weather_location_grid['LOCATION_KEY'].unique())

len(location_grid_stations['KEY'].unique())

file_path = root_path + f'{province} Location/{province}_weather_location_grid.csv.gz'
weather_location_grid.to_csv(file_path, compression='gzip', index=False)

# Download the file locally 
#files.download(file_path)