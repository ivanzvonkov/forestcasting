# -*- coding: utf-8 -*-
"""5-join_datasets.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i45c1n0Q53m7Tx_-Dm-cTiTO0a02or1X

# Join Datasets

Inputs:

(1) Base_dataset

(2) List of datasets to merge to base

(3) Columns to merge on

Output:

{joint_dataset}.csv
"""

# Imports
import pandas as pd
import numpy as np
import io
from google.colab import files, drive
import zipfile
import gzip
import shutil

# Mount Google Drive
drive.mount('/content/gdrive')

province = 'BC'
# Add your root path here
root_path = '/content/gdrive/My Drive/Capstone Public Folder/Data/'

jointdataset = f"{province}_dataset"
# base dataset all other datasets are merged with (ie. left)
base_dataset = {"path": f"{province} Location/{province}_init_dataset.csv.gz"}

# list of datasets to be merged to base. "key" = column of dataset specifed in path to merge on (ie. right key)
# base_key = column of base_dataset to match it to (ie. left key)
# Historic stats are merged on right because they contain some non existing grids
datasets = [{"path": f"{province} Location/{province}_data_with_eco.csv.gz", "key": "KEY", "base_key": "LOCATION_KEY", "how": "inner"},
            {"path": f"{province} Location/{province}_historic_stats.csv.gz", "key":"LOCATION_YEAR_KEY", "base_key": "LOCATION_YEAR_KEY", "how": "inner"}]

# For debugging
historic = pd.read_csv(root_path + f"{province} Location/{province}_historic_stats.csv.gz")
historic
eco = pd.read_csv(root_path + f"{province} Location/{province}_data_with_eco.csv.gz")
eco
init =  pd.read_csv(root_path + f"{province} Location/{province}_init_dataset.csv.gz")
init

len(historic)

len(eco)

len(init)

len(merged_df)

# Left merge to combine datasets
merged_df =  pd.read_csv(root_path + base_dataset["path"], compression = 'gzip')
for data in datasets:
  data_df = pd.read_csv(root_path + data["path"], compression = 'gzip')
  merged_df = pd.merge(merged_df, data_df, how=data["how"], left_on = data["base_key"], right_on = data["key"])

del merged_df['KEY']
merged_df.head()

null_columns=merged_df.columns[merged_df.isnull().any()]
merged_df[null_columns].isnull().sum()

# Save file to google drive
output_path = root_path + f'{province} Location/{jointdataset}.csv.gz'
merged_df.to_csv(output_path, compression = "gzip", index = False)

len(merged_df)

# Download the file locally 
#files.download(f'{jointdataset}.csv.gz')