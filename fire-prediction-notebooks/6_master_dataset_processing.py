# -*- coding: utf-8 -*-
"""6-master_dataset_processing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fPkOKqBWn4ow70ifixRjPOyCorKH0D4o

# Master Dataset Processing
**Input: {province}_dataset.csv.gz**
*   Full dataset of features for given province

**Output: {province}_processed_sample_dataset.csv.gz**

*   Split 40/60 fire/no fire
*   Null and NaN values filled
*   Create new features to better represent data
*   Create bins for continous values
"""

# Set province 
province = 'ON'
root_path = '/content/gdrive/My Drive/Capstone Public Folder/Data/'

# Commented out IPython magic to ensure Python compatibility.
# Imports
import pandas as pd
import numpy as np
import io
from google.colab import files, drive
import zipfile
import gzip
import shutil
from math import isnan, nan, exp
import datetime
import time
from sklearn.preprocessing import MinMaxScaler, StandardScaler

# visualization
import seaborn as sns
import matplotlib.pyplot as plt

# %matplotlib inline

# Mount Google Drive
drive.mount('/content/gdrive')

# Master Dataset Path
master_path = root_path + f'{province} Location/{province}_dataset.csv.gz'
master_df = pd.read_csv(master_path, compression='gzip')

master_df.info()

master_df.drop(['HEAT_DEG_DAYS', 'COOL_DEG_DAYS', 'LOCATION_YEAR_KEY', 'CENTROID', 'hourly'], axis=1, inplace=True) # delete location key and datetime later

# Make 40 fire / 60 no fire split
# Skip split
sample_df = master_df

# fire_df = master_df[master_df['FIRE'] == 1]
# no_fire_df = master_df[master_df['FIRE'] == 0]
# fire_amount = len(fire_df)
# no_fire_amount = len(no_fire_df)
# desired_no_fire_amount = fire_amount*(0.6/0.4)
# desired_no_fire_fraction = desired_no_fire_amount / no_fire_amount
# no_fire_sample_df = no_fire_df.sample(frac=desired_no_fire_fraction)
# sample_df = pd.concat([fire_df, no_fire_sample_df], axis=0)
# sample_df.describe()

"""### Describing Features
* **Max/Mean/Min Temp**: Continous numerical
* **Total Rain/Total Precipitation/Total Snow/Snow on Grnd**: Continous numerical
* **Dir of Max Gust**: Continous numerical
* **Speed of Max Gust**: Combination of data types
* **Fire**: Categorical nominal
"""

# Which features contain null or NaN values
null_columns=sample_df.columns[sample_df.isnull().any()]
sample_df[null_columns].isnull().sum()

"""### Part 1: How to fill in null values?
* **Max/Mean/Min Temp**: If some value is available use it, else compare days before and after and use average between two
* **Total Rain/Total Precipitation/Total Snow/Snow on Grnd**: Put 0 for all NaN values
* **Dir of Max Gust**: If speed is 0 this is also 0 else random
* **Speed of Max Gust**: If NaN make it 0, if some value with '<' remove the '<'
"""

# Fill NaN with 0 where applicable
cols = ['TOTAL_RAIN', 
        'TOTAL_PRECIP', 
        'TOTAL_SNOW',
        'SNOW_ON_GRND',
        'SPD_OF_MAX_GUST',
        'TOTAL_SIZE_HA',
        'AVERAGE_SIZE_HA',
        'TOTAL_DURATION',
        'AVERAGE_DURATION'
      ]

sample_df[cols] = sample_df[cols].fillna(value = 0);

def get_location_date(location_date_key):
  location_date_key = location_date_key.split('|')
  location = '|'.join(location_date_key[:2])
  row_date = location_date_key[2]
  date_time_obj = datetime.datetime.strptime(row_date, '%Y-%m-%d')
  return (location, date_time_obj)  

def day_before_key(location_date_key):
  location, date_time_obj = get_location_date(location_date_key)
  date_before = (date_time_obj - datetime.timedelta(days=1)).date()
  location_date_key_before = location + '|' + str(date_before)
  return location_date_key_before

def day_after_key(location_date_key):
  location, date_time_obj = get_location_date(location_date_key)
  date_after = (date_time_obj + datetime.timedelta(days=1)).date()
  location_date_key_after = location + '|' + str(date_after)
  return location_date_key_after

sample_df['LOCATION_DATE_BEFORE_KEY'] = np.vectorize(day_before_key)(sample_df['LOCATION_DATE_KEY'])
sample_df['LOCATION_DATE_AFTER_KEY'] = np.vectorize(day_after_key)(sample_df['LOCATION_DATE_KEY'])

master_df['LOCATION_DATE_KEY_INDEX'] = master_df['LOCATION_DATE_KEY']
master_df.set_index('LOCATION_DATE_KEY_INDEX', inplace=True)
master_df.tail()

def fill_temp_using_adjacent_days(key, val, ldk_before, ldk_after):
  try:
    yesterday_val = master_df.at[ldk_before, key]
    tomorrow_val = master_df.at[ldk_after, key]
    if not isnan(yesterday_val) and not isnan(tomorrow_val):
      val = (yesterday_val + tomorrow_val)/2
    return val
  except:
    return val

# calculate averages
keys = ['MEAN_TEMP', 'MIN_TEMP', 'MAX_TEMP', 'TEMP_12_4', 'DEW_TEMP_12_4', 'REL_HUM_12_4']
averages = {}
for key in keys:
  start = time.time()
  print(f'Processing {key}')
  sample_df[key] = np.vectorize(fill_temp_using_adjacent_days)(key, sample_df[key], sample_df['LOCATION_DATE_BEFORE_KEY'], sample_df['LOCATION_DATE_AFTER_KEY'])
  end = time.time()
  print(end-start)
  averages[key] = master_df[key].mean()

def fill_mean_temp(mean_temp, max_temp, min_temp):
  if not isnan(mean_temp):
    return mean_temp

  if not isnan(max_temp) and not isnan(min_temp):
    mean_temp = (max_temp + min_temp)/2
  elif not isnan(min_temp):
    mean_temp = min_temp + 5
  elif not isnan(max_temp):
    mean_temp = max_temp - 5
  else:
    mean_temp = averages['MEAN_TEMP']
  
  return mean_temp

def fill_min_temp(mean_temp, max_temp, min_temp):
  if not isnan(min_temp):
    return min_temp

  if not isnan(mean_temp):
    min_temp = mean_temp - 5
  elif not isnan(max_temp):
    min_temp = max_temp - 10
  else:
    min_temp = averages['MIN_TEMP']
  
  return min_temp

def fill_max_temp(mean_temp, max_temp, min_temp):
  if not isnan(max_temp):
    return max_temp

  if not isnan(mean_temp):
    max_temp = mean_temp + 5
  elif not isnan(min_temp):
    max_temp = min_temp + 10
  else:
    max_temp = averages['MAX_TEMP']
  
  return max_temp 

def fill_temp_12_4(temp_12_4, max_temp):
  if not isnan(temp_12_4):
    return temp_12_4

  if not isnan(max_temp):
    temp_12_4 = max_temp
  else:
    temp_12_4 = averages['TEMP_12_4']

  return temp_12_4

def fill_dew_temp_12_4(dew_temp_12_4):
  if not isnan(dew_temp_12_4):
    return dew_temp_12_4
  else:
    return averages['DEW_TEMP_12_4']

def fill_rel_hum_12_4(rel_hum_12_4):
  if not isnan(rel_hum_12_4):
    return rel_hum_12_4
  else:
    return averages['REL_HUM_12_4']

sample_df['MEAN_TEMP'] = np.vectorize(fill_mean_temp)(sample_df['MEAN_TEMP'], sample_df['MAX_TEMP'], sample_df['MIN_TEMP'])
sample_df['MIN_TEMP'] = np.vectorize(fill_min_temp)(sample_df['MEAN_TEMP'], sample_df['MAX_TEMP'], sample_df['MIN_TEMP'])
sample_df['MAX_TEMP'] = np.vectorize(fill_max_temp)(sample_df['MEAN_TEMP'], sample_df['MAX_TEMP'], sample_df['MIN_TEMP'])
sample_df['TEMP_12_4'] = np.vectorize(fill_temp_12_4)(sample_df['TEMP_12_4'], sample_df['MAX_TEMP'])
sample_df['DEW_TEMP_12_4'] = np.vectorize(fill_dew_temp_12_4)(sample_df['DEW_TEMP_12_4'])
sample_df['REL_HUM_12_4'] = np.vectorize(fill_rel_hum_12_4)(sample_df['REL_HUM_12_4'])

# Fix wind values 
def fix_wind(spd_of_max_gust):
  try:
   return int(spd_of_max_gust)
  except:
    return int(float(spd_of_max_gust[1:]))
  return row
  
sample_df['SPD_OF_MAX_GUST'] = np.vectorize(fix_wind)(sample_df['SPD_OF_MAX_GUST'])

# Sub in -1 for max gust if empty
sample_df['DIR_OF_MAX_GUST'] = sample_df['DIR_OF_MAX_GUST'].fillna(value = -1);

# Verify null values 
null_columns=sample_df.columns[sample_df.isnull().any()]
sample_df[null_columns].isnull().sum()

# Empty series means no nan values

"""## Part 2: Assumptions
* **Max/Mean/Min Temp/Temp 12-4**: Higher temperature correlates with higher chance of fire.

* **Total Rain/Total Precipitation/Total Snow/Snow on Grnd**: Must all be zero for fire to be possibe

* **Dir of Max Gust**: Direction most likely has no impact
* **Speed of Max Gust**: Higher wind correlates with fire occurance

* **Dew point temp 12-4**: Higher dew point temperature correlates with hotter weather and higher fire occurence

* **Relative humidity 12-4**: Higher relative humidity correlates higher change of fire

## Part 3: Correlations and Conclusions

### 3a: Temperatures
Confirms assumption of higher temperature resulting in higher fire chance. Will keep all temperaure values as Min, Mean, and Max have different distributions.
"""

# Correlations between temperatures and label
g = sns.FacetGrid(sample_df, col='FIRE')
g.map(plt.hist, 'MEAN_TEMP', bins=10)

g = sns.FacetGrid(sample_df, col='FIRE')
g.map(plt.hist, 'MAX_TEMP', bins=10)

g = sns.FacetGrid(sample_df, col='FIRE')
g.map(plt.hist, 'MIN_TEMP', bins=10)

g = sns.FacetGrid(sample_df, col='FIRE')
g.map(plt.hist, 'TEMP_12_4', bins=10)

"""### 3b: Rain and Precipitation
1. Fires can still occur when rain is present.
2. lot's of precipitation (ie over 20 mm) looks results in no fires. Will need to ensure bins of precipitation account for this.
"""

# Correlations between precipation and label
g = sns.FacetGrid(sample_df, col='FIRE')
g.map(plt.hist, 'TOTAL_RAIN')

# Precipitation is more general
g = sns.FacetGrid(sample_df, col='FIRE')
g.map(plt.hist, 'TOTAL_PRECIP')

"""### 3c: Snow
Fires only occur when snow on ground and total snow is 0. Can create feature IS_SNOW_ON_GRND and IS_SNOW_FALLING
"""

g = sns.FacetGrid(sample_df, col='FIRE')
g.map(plt.hist, 'TOTAL_SNOW')

g = sns.FacetGrid(sample_df, col='FIRE')
g.map(plt.hist, 'SNOW_ON_GRND')

"""### 3d: Wind
The big gap between 5-30 with wind speed is due to all values such as '<30' were converted to simply '30'.
Overall direction of gust does not seem to impact the fire occurrence. Speed of max gust also shows a similar distribution with or without fire meaning no correlation.
"""

g = sns.FacetGrid(sample_df, col='FIRE')
g.map(plt.hist, 'SPD_OF_MAX_GUST')

g = sns.FacetGrid(sample_df, col='FIRE')
g.map(plt.hist, 'DIR_OF_MAX_GUST')

"""### 3e: Rel Hum and Dew Point
Dew point temp must be above 0 for fire to happen. 

No strong correlation between relative humidity. Absolute humidity should be calculated for more a useful feature.
"""

g = sns.FacetGrid(sample_df, col='FIRE')
g.map(plt.hist, 'DEW_TEMP_12_4')

g = sns.FacetGrid(sample_df, col='FIRE')
g.map(plt.hist, 'REL_HUM_12_4')

"""### 3f: Historic Stats
Only TOTAL_DURATION_OLD has some correlation with the occurence of fires.
"""

g = sns.FacetGrid(sample_df, col='FIRE')
g.map(plt.hist, 'TOTAL_SIZE_HA_OLD')

g = sns.FacetGrid(sample_df, col='FIRE')
g.map(plt.hist, 'AVERAGE_SIZE_HA_OLD')

g = sns.FacetGrid(sample_df, col='FIRE')
g.map(plt.hist, 'TOTAL_DURATION_OLD')

g = sns.FacetGrid(sample_df, col='FIRE')
g.map(plt.hist, 'AVERAGE_DURATION_OLD')

"""## Part 4: Creating new features

### 4a: Snow features
Based on conclusions will create feature IS_SNOW_ON_GROUND , IS_SNOW_FALLING
"""

# Fill empty dates 
sample_df['IS_SNOW_ON_GROUND'] = sample_df['SNOW_ON_GRND'] > 0
sample_df['IS_SNOW_FALLING'] = sample_df['TOTAL_SNOW'] > 0

"""### 4b Humidity features
Create absolute humidity feature

Based on: https://carnotcycle.wordpress.com/2012/08/04/how-to-convert-relative-humidity-to-absolute-humidity/
"""

def create_abs_humidity(t, rh):
  abs_hum = (6.112 * exp((17.67*t)/(t+243.5)) * rh * 2.1674)/(273.15+t)
  return abs_hum

sample_df['ABS_HUM_12_4'] = np.vectorize(create_abs_humidity)(sample_df['TEMP_12_4'], sample_df['REL_HUM_12_4'])

g = sns.FacetGrid(sample_df, col='FIRE')
g.map(plt.hist, 'ABS_HUM_12_4')

"""## 4c Month and Day Features
Make cyclical month and day features.
"""

def addMonth(date):
  date_time_obj = datetime.datetime.strptime(date, '%Y-%m-%d')
  return date_time_obj.month  

def addDay(date):
  date_time_obj = datetime.datetime.strptime(date, '%Y-%m-%d')
  return date_time_obj.day   

sample_df['MONTH'] = np.vectorize(addMonth)(sample_df['DATETIME'])
sample_df['DAY'] = np.vectorize(addDay)(sample_df['DATETIME'])
sample_df['MONTH_SIN'] = np.sin((sample_df['MONTH']-1)*(2.*np.pi/12))
sample_df['MONTH_COS'] = np.cos((sample_df['MONTH']-1)*(2.*np.pi/12))
sample_df['DAY_SIN'] = np.sin((sample_df['DAY']-1)*(2.*np.pi/31))
sample_df['DAY_COS'] = np.cos((sample_df['DAY']-1)*(2.*np.pi/31))
sample_df = sample_df.drop(columns=['MONTH', 'DAY', 'DATETIME'])
sample_df

"""### Optimizing features
Create one hot encoding for eco zone, district, and region
"""

# Create one hot encoding for eco zone, district, and region
# Will make dummies in 7
# eco_labels = ['ECOZONE', 'ECOREGION', 'ECODISTRICT']
# sample_df = pd.get_dummies(sample_df, columns=eco_labels)
# sample_df

"""**EDIT: Bins discontinued, will scale instead**

Make bins for continous features:
1. Temperatures
2. Precipitation
3. Stats
"""

# # Bin temperature
# for temp_label in ['MEAN_TEMP', 'MAX_TEMP', 'MIN_TEMP', 'TEMP_12_4', 'DEW_POINT_TEMP_12_4']:
#   sample_df['QUANTILE_'+temp_label] = pd.qcut(sample_df[temp_label], q=8, precision=0, duplicates='drop', labels=False)

# # Bin precipitation, rain
# for temp_label in ['TOTAL_RAIN', 'TOTAL_PRECIP']:
#   sample_df['QUANTILE_'+temp_label] = pd.qcut(sample_df[temp_label], q=5, precision=0, duplicates='drop', labels=False)

# # Bin relative humidity
# sample_df['QUANTILE_REL_HUM_12_4'] = pd.qcut(sample_df['REL_HUM_12_4'], q=10, precision=0, duplicates='drop', labels=False)

# # Bin absolute humidity
# sample_df['QUANTILE_ABS_HUM_12_4'] = pd.qcut(sample_df['ABS_HUM_12_4'], q=5, precision=0, duplicates='drop', labels=False)

# for stats_label in ['TOTAL_SIZE_HA_OLD', 'TOTAL_SIZE_HA_OLD', 'TOTAL_DURATION_OLD', 'AVERAGE_DURATION_OLD']:
#   sample_df['QUANTILE_'+stats_label] = pd.qcut(sample_df[stats_label], q=5, precision=0, duplicates='drop', labels=False)

"""Scale continous features"""

# Scale continous features
# Not using standard scaler because size, duration is not normal
# scaler = MinMaxScaler()
# columns_to_scale = [
#   'MEAN_TEMP', 'MAX_TEMP', 'MIN_TEMP', 'TEMP_12_4', 'DEW_POINT_TEMP_12_4',
#   'TOTAL_RAIN', 'TOTAL_PRECIP', 'REL_HUM_12_4', 'ABS_HUM_12_4',
#   'TOTAL_SIZE_HA_OLD', 'TOTAL_SIZE_HA_OLD', 'TOTAL_DURATION_OLD', 'AVERAGE_DURATION_OLD'
#   ]

# scaled_column_names = ['SCALED_' + s for s in columns_to_scale]
# scaled_df = sample_df.copy()
# scaled_df[columns_to_scale] = scaler.fit_transform(sample_df[columns_to_scale])
# scaled_df

"""## Export"""

sample_df = sample_df.drop(columns=['LOCATION_KEY', 'Month','LOCATION_DATE_BEFORE_KEY','LOCATION_DATE_AFTER_KEY'])
sample_df

sample_df.to_csv(root_path + f'{province} Location/{province}_processed_sample_dataset.csv.gz', compression='gzip', index=False)