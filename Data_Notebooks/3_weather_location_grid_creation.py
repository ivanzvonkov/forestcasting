# -*- coding: utf-8 -*-
"""3-weather_location_grid_creation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EUBcKqA4nywMagGbdbapXsnEyy252tq6

## Creating weather location grid table
Inputs:
- {province}_location_grid_wooded.csv.zip
- {province}_location_grid_usefulness.csv.zip
- {province_lowercase}_grid_system_with_station.csv.zip
- current_station_inventory.csv.zip

1. Import existing grid files.
2. Eliminate non-useful grids.
3. Create new dataframe by iterating through grids and weather for that specific grid.
"""

province = 'AL'

root_path = '/content/gdrive/My Drive/Capstone Public Folder/Data/'

# Imports
import pandas as pd
import numpy as np
import io
from google.colab import files, drive
import zipfile
import shutil
import time
import requests
from io import StringIO

# Mount Google Drive
drive.mount('/content/gdrive')

required_files = [
  f'{province} Location/Ivan_NarrowedData/{province}_location_grid_wooded.csv.zip',
  f'{province} Location/Shima_NarrowedData/{province}_location_grid_usefulness.csv.zip'
]

for local_path in required_files:
  file_path = root_path + local_path
  with zipfile.ZipFile(file_path,"r") as zip_ref:
    zip_ref.extractall('.')

# Import all files into pandas dataframes
location_grid_wooded = pd.read_csv(f'{province}_location_grid_wooded.csv')
location_grid_boreal = pd.read_csv(f'{province}_location_grid_usefulness.csv')
location_grid_stations = pd.read_csv(root_path + f'{province} Location/{province}_grid_system_with_station.csv.gz')
weather_stations = pd.read_csv(root_path + 'Weather - Stations/current_station_inventory.csv.gz')

# Gets weather data for using station IDs in location_grid_stations.csv
start_year = 2017
end_year = 2018
df = pd.DataFrame({})
index = 0
station_list = location_grid_stations['STATION_ID'].unique()
for stationID in station_list:
    index += 1
    start = time.time( )
    for year in range(start_year, end_year+1):
        url = "http://climate.weather.gc.ca/climate_data/bulk_data_e.html?format=csv&" +\
            f"stationID={stationID}&Year={year}&Month=1&Day=14" + "&timeframe=2&submit=Download+Data"
        response = requests.get(url)
        string_data = str(response.content, 'utf-8')
        data = StringIO(string_data)
        new_df = pd.read_csv(data)
        new_df['STATION_ID'] = stationID
        if len(df.index) == 0:
            df = new_df
        else:
            df = pd.concat([df, new_df], ignore_index=True)
    end = time.time()
    total_time = end - start
    print('Got data from ' + str(index) + ' of ' + str(len(station_list)) +
          ' weather stations in: ' + str(total_time))

# Narrows weather data by months
filter_months = [7,8,9]
if len(filter_months) > 0:
    df = df.loc[df['Month'].isin(filter_months)]

daily_weather_station_data = df
daily_weather_station_data

# Get hourly weather data
index = 0
hourly_weather_data = pd.DataFrame({})
for stationID in station_list:
  index += 1
  start = time.time()
  for year in range(start_year, end_year+1):
    for month in filter_months:
      url = "http://climate.weather.gc.ca/climate_data/bulk_data_e.html?format=csv&" +\
              f"stationID={stationID}&Year={year}&Month={month}&Day=14" + "&timeframe=1&submit=Download+Data"
      response = requests.get(url)
      string_data = str(response.content, 'utf-8')
      data = StringIO(string_data)
      new_df = pd.read_csv(data)
      new_df['STATION_ID'] = stationID
      if len(hourly_weather_data.index) == 0:
          hourly_weather_data = new_df
      else:
          hourly_weather_data = pd.concat([hourly_weather_data, new_df], ignore_index=True)
  end = time.time()
  total_time = end - start
  print('Got data from ' + str(index) + ' of ' + str(len(station_list)) +
        ' weather stations in: ' + str(total_time))

# Only keep hot times (12:00 - 4:00 pm)
filter_times = ['12:00', '13:00', '14:00', '15:00', '16:00']
if len(filter_times) > 0:
  hourly_weather_data = hourly_weather_data.loc[hourly_weather_data['Time'].isin(filter_times)]

def add_hourly_averages(weather_row):
  hourly_weather_specific_day = hourly_weather_data.loc[
    (hourly_weather_data['Year'] == weather_row['Year']) &
    (hourly_weather_data['Month'] == weather_row['Month']) &
    (hourly_weather_data['Day'] == weather_row['Day']) &
    (hourly_weather_data['STATION_ID'] == weather_row['STATION_ID']) 
  ]
  
  if len(hourly_weather_specific_day) != 5:
    print(f'WARNING: Should have found five data points for {weather_row["Year"]}-{weather_row["Month"]}-{weather_row["Day"]} for station: {weather_row["STATION_ID"]}')
    print(hourly_weather_data)
  weather_row['TEMP_12_4'] = hourly_weather_specific_day['Temp (°C)'].mean()
  weather_row['DEW_POINT_TEMP_12_4'] = hourly_weather_specific_day['Dew Point Temp (°C)'].mean()
  weather_row['REL_HUM_12_4'] = hourly_weather_specific_day['Rel Hum (%)'].mean()
  return weather_row

daily_weather_station_data = daily_weather_station_data.apply(lambda row: add_hourly_averages(row), axis=1)
daily_weather_station_data

# Get daily weather ready for merging
daily_weather_with_station = pd.merge(weather_stations, daily_weather_station_data, on=['STATION_ID'])
daily_weather_with_station

# Problem weather stations
unmatched = daily_weather_with_station.loc[daily_weather_with_station['Station Name_x'] != daily_weather_with_station['Station Name_y']]
unmatched[['Station Name_x', 'Station Name_y', 'STATION_ID']].drop_duplicates(keep='last')

weather_stations.loc[(weather_stations['Station Name'] == 'TROUT LAKE') | (weather_stations['Station Name'] == "SAMBAA K'E")]
# Conclusion SAMBAA K'E is a mystery station name that should be removed

daily_weather_with_station = daily_weather_with_station.rename({'Station Name_x': 'STATION_NAME'}, axis=1)
daily_weather_with_station = daily_weather_with_station.drop(['Climate ID_x', 'Climate ID_y', 'Station Name_y'], axis=1)
daily_weather_with_station

# Find weather stations that have empty values
all_condition = True
or_condition = False
for col in [
 'Max Temp (°C)',
 'Min Temp (°C)',
 'Mean Temp (°C)',
 'Heat Deg Days (°C)',
 'Cool Deg Days (°C)',
 'Total Rain (mm)',
 'Total Snow (cm)',
 'Total Precip (mm)',
 'Snow on Grnd (cm)',
 'Dir of Max Gust (10s deg)',
 'Spd of Max Gust (km/h)']:
  is_val_null = daily_weather_with_station[col].isnull()
  all_condition = all_condition & is_val_null
  or_condition = or_condition | is_val_null

weather_stations_missing_some_data = daily_weather_with_station[or_condition]['STATION_ID'].value_counts().reset_index().rename({'index': 'STATION_ID', 'STATION_ID': 'AMOUNT_SOME_MISSING'}, axis=1)
weather_stations_missing_all_data = daily_weather_with_station[all_condition]['STATION_ID'].value_counts().reset_index().rename({'index': 'STATION_ID', 'STATION_ID': 'AMOUNT_ALL_MISSING'}, axis=1)
print(f'Total weather stations: {len(daily_weather_with_station["STATION_ID"].unique())}')

weather_stations_missing_all_data

weather_stations_missing_some_data

# Conclusion - all weather stations are missing some data
# However for weather stations missing over 90 days of ALL data, we'll use backup weather
bad_stations = weather_stations_missing_all_data.loc[weather_stations_missing_all_data['AMOUNT_ALL_MISSING'] > 90]['STATION_ID']

def is_backup_needed(row):
  if row['STATION_ID'] in bad_stations.unique() and row['BACKUP_STATION_ID'] not in bad_stations.unique():
    row['STATION_ID'] = row['BACKUP_STATION_ID']
  return row

location_grid_stations_using_backup = location_grid_stations.apply(lambda row: is_backup_needed(row), axis=1)
bad_stations

for bad_station in bad_stations:
  if(len(location_grid_stations_using_backup[location_grid_stations_using_backup['STATION_ID'] == bad_station]) > 0):
    print(f'Bad_station: {bad_station} dooes not have replacement.')

# Leave only useful grids
#print(f'Total grids: {len(location_grid_stations)}')
#only_wooded_grid_locations = location_grid_wooded[location_grid_wooded['wooded'] == 1]
#useful_grids =  pd.merge(location_grid_boreal, only_wooded_grid_locations, on='KEY')['KEY']
#print(f'Useful grids: {len(useful_grids)}')

# DON'T ELIMINATE ANY GRIDS
useful_location_grid_stations = location_grid_stations_using_backup #pd.merge(useful_grids, location_grid_stations, on='KEY')
#print(f'Eliminated: {len(location_grid_stations) - len(useful_location_grid_stations)}')

weather_location_grid = pd.merge(useful_location_grid_stations, daily_weather_with_station, on='STATION_ID')
weather_location_grid.head()

# Clean up table
weather_location_grid.rename(columns={'KEY': 'LOCATION_KEY'}, inplace=True)
weather_location_grid['LOCATION_DATE_KEY'] = weather_location_grid['LOCATION_KEY'] + '|' + weather_location_grid['Date/Time']
weather_location_grid.head()

weather_location_grid.describe()

file_path = root_path + f'{province} Location/{province}_weather_location_grid.csv.gz'
weather_location_grid.to_csv(file_path, compression='gzip', index=False)

# Download the file locally 
files.download('weather_location_grid.csv.gz')