# -*- coding: utf-8 -*-
"""weather_location_grid_creation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EUBcKqA4nywMagGbdbapXsnEyy252tq6

## Creating weather location grid table
1. Import existing grid files.
2. Eliminate non-useful grids.
3. Create new dataframe by iterating through grids and weather for that specific grid.
"""

# Imports
import pandas as pd
import numpy as np
import io
from google.colab import files, drive
import zipfile
import shutil
import time
import requests
from io import StringIO

# Mount Google Drive
drive.mount('/content/gdrive')

# Set root path, will be different for everyone
root_path = '/content/gdrive/My Drive/U - SE 4455 Software Design/Capstone Public Folder/Data/'

required_files = [
  'Alberta Location /Ivan_NarrowedData/AL_location_grid_wooded.csv.zip',
  'Alberta Location /Shima_NarrowedData/AL_location_grid_usefulness.csv.zip',
  'Weather - Stations/alberta_grid_system_with_station.csv.zip',
  'Weather - Stations/current_station_inventory.csv.zip',
]

for local_path in required_files:
  file_path = root_path + local_path
  with zipfile.ZipFile(file_path,"r") as zip_ref:
    zip_ref.extractall('.')

# Import all files into pandas dataframes
location_grid_wooded = pd.read_csv('AL_location_grid_wooded.csv')
location_grid_boreal = pd.read_csv('AL_location_grid_usefulness.csv')
location_grid_stations = pd.read_csv('alberta_grid_system_with_station.csv')
weather_stations = pd.read_csv('current_station_inventory.csv')

# Gets weather data for using station IDs in location_grid_stations.csv
start_year = 2017
end_year = 2018
df = pd.DataFrame({})
index = 0
station_list = location_grid_stations['STATION ID'].unique()
for stationID in station_list:
    index += 1
    start = time.time( )
    for year in range(start_year, end_year+1):
        url = "http://climate.weather.gc.ca/climate_data/bulk_data_e.html?format=csv&" +\
            f"stationID={stationID}&Year={year}&Month=1&Day=14" + "&timeframe=2&submit=Download+Data"
        response = requests.get(url)
        string_data = str(response.content, 'utf-8')
        data = StringIO(string_data)
        new_df = pd.read_csv(data)
        if len(df.index) == 0:
            df = new_df
        else:
            df = pd.concat([df, new_df], ignore_index=True)
    end = time.time()
    total_time = end - start
    print('Got data from ' + str(index) + ' of ' + str(len(station_list)) +
          ' weather stations in: ' + str(total_time))

# Narrows weather data by months
filter_months = [7,8,9]
if len(filter_months) > 0:
    df = df.loc[df['Month'].isin(filter_months)]

daily_weather_station_data = df

# Leave only useful grids
print(f'Total grids: {len(location_grid_stations)}')
only_wooded_grid_locations = location_grid_wooded[location_grid_wooded['wooded'] == 1]
useful_grids = pd.merge(location_grid_boreal, only_wooded_grid_locations, on='KEY')['KEY']
print(f'Useful grids: {len(useful_grids)}')

useful_location_grid_stations = pd.merge(useful_grids, location_grid_stations, on='KEY')
print(f'Eliminated: {len(location_grid_stations) - len(useful_location_grid_stations)}')

# Rename key name
weather_stations.rename(columns={'Name': 'Station Name', 'Station ID': 'STATION ID'}, inplace=True)
weather_stations = weather_stations[['Station Name', 'STATION ID']]

# Get daily weather ready for merging
daily_weather_with_station = pd.merge(weather_stations, daily_weather_station_data, on='Station Name')
daily_weather_with_station.head()

# Find weather stations that have empty values
condition = daily_weather_station_data['Data Quality'].isnull();
for col in ['Data Quality',
 'Max Temp (°C)',
 'Max Temp Flag',
 'Min Temp (°C)',
 'Min Temp Flag',
 'Mean Temp (°C)',
 'Mean Temp Flag',
 'Heat Deg Days (°C)',
 'Heat Deg Days Flag',
 'Cool Deg Days (°C)',
 'Cool Deg Days Flag',
 'Total Rain (mm)',
 'Total Rain Flag',
 'Total Snow (cm)',
 'Total Snow Flag',
 'Total Precip (mm)',
 'Total Precip Flag',
 'Snow on Grnd (cm)',
 'Snow on Grnd Flag',
 'Dir of Max Gust (10s deg)',
 'Dir of Max Gust Flag',
 'Spd of Max Gust (km/h)',
 'Spd of Max Gust Flag']:
  condition = condition & daily_weather_station_data[col].isnull()

weather_stations_missing_data = daily_weather_station_data[condition]['Station Name'].value_counts().reset_index().rename(columns={'index': 'Station Name', 0: 'count'})

weather_stations_missing_data

weather_location_grid = pd.merge(useful_location_grid_stations, daily_weather_with_station, on='STATION ID')
weather_location_grid.head()

# Clean up table
weather_location_grid.rename(columns={'KEY': 'LOCATION KEY'}, inplace=True)
del weather_location_grid['Unnamed: 0']
weather_location_grid['KEY'] = weather_location_grid['LOCATION KEY'] + '|' + weather_location_grid['Date/Time']

weather_location_grid.set_index('KEY', inplace=True)
weather_location_grid.head()

weather_location_grid.describe()

weather_location_grid.to_csv('weather_location_grid.csv.gz', compression='gzip')

# Download the file locally 
files.download('weather_location_grid.csv.gz')

# Save file to google drive
file_path = root_path + 'Alberta Location /AL_weather_location_grid.csv.gz'
shutil.copy('weather_location_grid.csv.gz', file_path)