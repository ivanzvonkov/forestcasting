# -*- coding: utf-8 -*-
"""6a-join_fire_weather.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Kb0dc3BbABra1XqGj0swQ9dcjtEpjIXg

# Join Fire & Weather Data
**Inputs:**

(1) Weather location data: {province}_weather_location_grid.csv

(2) Fire location data: {province}_fire_location_grid.csv

**Output**: 

{province}_dataset.csv
"""

# Set province 
province = 'AL'

# Imports
import pandas as pd
import numpy as np
import io
from google.colab import files, drive
import zipfile
import gzip
import shutil

# Mount Google Drive
drive.mount('/content/gdrive')

# Shima's root path
#root_path = '/content/gdrive/My Drive/Capstone Public Folder/Data/'

# Ivan's root path
root_path = '/content/gdrive/My Drive/U - SE 4455 Software Design/Capstone Public Folder/Data/'

# Access fire data gzip directly
fire_file_path = root_path + 'Forest Fire Data/AL_fire_location_grid.csv.gz'
fire_df = pd.read_csv(fire_file_path, compression='gzip')
fire_df.head()

# Access weather data gzip directly
weather_file_path = root_path + 'Alberta Location /AL_weather_location_grid.csv.gz'
weather_df = pd.read_csv(weather_file_path, compression='gzip')
weather_df.head()

# Create new key in fire_df : LOCATION + DATE
fire_df["LOCATION_DATE_KEY"] = fire_df["locationKey"].map(str) + "|" + fire_df["START_DATE"]
fire_df.head()

# Delete unused columns
cols_to_delete = ['LATITUDE', 
                  'LONGITUDE', 
                  'OUT_DATE', 
                  'SIZE_HA', 
                  'START_DATE', 
                  'ECODISTRIC', 
                  'ECOREGION', 
                  'ECOZONE', 
                  'locationKey']
for i in cols_to_delete:
  del fire_df[i]

fire_df

# Remove unnecessary columns from weather df
 cols_to_delete = ['STATION ID', 
                   'LOCATION KEY', 
                   'Date/Time', 
                   'Station Name', 
                   'Longitude (x)', 
                   'Latitude (y)', 
                   'Climate ID',
                   'Year', 
                   'Month', 
                   'Day', 
                   'Data Quality', 
                   'Max Temp Flag', 
                   'Min Temp Flag', 
                   'Mean Temp Flag',
                   'Heat Deg Days Flag', 
                   'Cool Deg Days Flag', 
                   'Total Rain Flag', 
                   'Total Snow Flag',
                   'Total Precip Flag', 
                   'Snow on Grnd Flag', 
                   'Dir of Max Gust Flag', 
                   'Spd of Max Gust Flag']

# Delete unused columns
for i in cols_to_delete:
  del weather_df[i]

weather_df

# Rename key column
weather_df.rename(columns={'KEY': "LOCATION_DATE_KEY"}, inplace = True)

# Standardize other columns
cols_to_rename = ['Max Temp (°C)',
                  'Min Temp (°C)',
                  'Mean Temp (°C)',
                  'Heat Deg Days (°C)',
                  'Cool Deg Days (°C)',
                  'Total Rain (mm)',
                  'Total Snow (cm)',
                  'Total Precip (mm)',
                  'Snow on Grnd (cm)',
                  'Dir of Max Gust (10s deg)',
                  'Spd of Max Gust (km/h)']

def standardize_col_names(col_name):
  name = col_name.split('(')[0].upper()
  names = name.split(' ')
  final =""
  j=0
  while j<len(names)-2:
    final += names[j]+'_'
    j=j+1
  final += names[j]    
  return final

for i in cols_to_rename:
  weather_df.rename(columns={i: standardize_col_names(i) }, inplace = True)

weather_df.head()

# Outer merge to combine fire and weather data
merged_df = pd.merge(fire_df, weather_df, how='outer')
merged_df

# Label fire and no fire rows - WORKS NOW
merged_df['FIRE'] = merged_df.apply(lambda row: 1 if isinstance(row['FIRE_ID'], str) else 0, axis=1)

merged_df.describe()

# Delete FIRE ID col
del merged_df['FIRE_ID']
merged_df.head()

# Compress csv
merged_df.to_csv(f'{province}_dataset.csv.gz', compression='gzip')

# Download the file locally 
files.download(f'{province}_dataset.csv.gz')

# Save file to google drive
file_path = root_path + f'Colab Data/{province}_dataset.csv.gz'
shutil.copy(f'{province}_dataset.csv.gz', file_path)