# -*- coding: utf-8 -*-
"""7-train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P_WHGSuLu1J4oHGFyg4o7oeMmht7Zsiq

# Train 
Inputs:
- {province}_processed_sample_dataset.csv.gz



A commonly recommended training/testing ratio is 70/30:


*   70 percent of the dataset is training data
*   30 percent of the dataset is testing data
"""

# Imports
import pandas as pd
import numpy as np
from google.colab import files, drive
from datetime import datetime
from os import path

# keras for dnn
from keras.layers.core import Dense, Activation
from keras.utils.np_utils import to_categorical
from keras.models import Sequential

# sklearn for simpler models
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC, LinearSVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import Perceptron
from sklearn.linear_model import SGDClassifier
from sklearn.tree import DecisionTreeClassifier

# Set Province
province = 'AL'
training_percent = 0.7

# Mount Google Drive
drive.mount('/content/gdrive')

# Set path to data folder
root_path = '/content/gdrive/My Drive/Capstone Public Folder/Data/'

# Read in results
results_path = root_path + f'Colab Data/{province}_results.csv.gz'
try:
  results_df = pd.read_csv(results_path, compression='gzip')
except:
  results_df = ''

# Processed Dataset Path
sample_path = root_path + f'Colab Data/{province}_processed_sample_dataset.csv.gz'
df = pd.read_csv(sample_path, compression='gzip')
df

# Create test and training sets
rows = len(df.index)
training_size = int(np.ceil(training_percent * rows))
testing_size = rows - training_size

train_df = df.sample(n=training_size)

cond = df['LOCATION_DATE_KEY'].isin(train_df['LOCATION_DATE_KEY']) == True
test_df = df.drop(df[cond].index, inplace = False)
print(f'Train df size: {len(train_df)}')
print(f'Test df size: {len(test_df)}')

train_df.set_index('LOCATION_DATE_KEY', inplace=True)
test_df.set_index('LOCATION_DATE_KEY', inplace=True)

X_train = train_df.drop("FIRE", axis=1)
Y_train = train_df["FIRE"]
X_test = test_df.drop("FIRE", axis=1)
Y_test = test_df["FIRE"]

# Setup sklearn tests
algorithms = {
  'LogisticRegression': LogisticRegression(),
  'SVC': SVC(),
  'LinearSVC': LinearSVC(),
  'RandomForestClassifier': RandomForestClassifier(),
  'KNeighborsClassifier': KNeighborsClassifier(),
  'GaussianNB': GaussianNB(),
  'Perceptron': Perceptron(),
  'SGDClassifier': SGDClassifier(),
  'DecisionTreeClassifier': DecisionTreeClassifier()  
}

# Test sklearn algorithms
results = {}
for key in algorithms:
  ml_algorithm = algorithms[key]
  ml_algorithm.fit(X_train, Y_train)
  Y_pred = ml_algorithm.predict(X_test)
  acc_log = round(ml_algorithm.score(X_test, Y_test) * 100, 2)
  results[key] = acc_log

# Visualize sklearn results without warnings
results

# Setup keras
def one_hot_encode_object_array(arr):
    uniques, ids = np.unique(arr, return_inverse=True)
    return to_categorical(ids, len(uniques))

Y_train_OHE = one_hot_encode_object_array(Y_train)
Y_test_OHE = one_hot_encode_object_array(Y_test)

# Set up model
hidden_units = 16
feature_num = X_train.shape[1]

model = Sequential()
model.add(Dense(hidden_units, input_shape=(feature_num,)))
model.add(Activation('sigmoid'))
model.add(Dense(2))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Run model
loss, accuracy = model.evaluate(X_test, Y_test_OHE, verbose=0)
results['keras'] = round(accuracy*100,2)
print(f'DNN accuracy: {accuracy*100}')
print(f'DNN loss: {loss}') # lower is better

# Add to results data table 
now = datetime.now()
results['DateTime'] = now.strftime("%Y-%m-%d %H:%M:%S")
if len(results_df) == 0:
  results_df = pd.DataFrame(results, index=[0])
else:
  results_df = results_df.append(results, ignore_index=True)

results_df

results_df.to_csv(results_path, compression='gzip', index=False)